{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6cdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a53bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/cmpe256/cmpe256_hotel_recommendation_system'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Setup paths ---\n",
    "current_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))  # Adjust to your project structure\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78264026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "input_path = os.path.join(current_dir, 'data', 'processed', 'hotelrec_2013_2017_cleaned_encoded.csv.gz')\n",
    "\n",
    "df = pd.read_csv(input_path, usecols=[\n",
    "    'author_id', 'hotel_name_id', 'rating', 'sentiment_score',\n",
    "    'sleep quality', 'value', 'rooms', 'service', 'cleanliness', 'location'\n",
    "])\n",
    "\n",
    "chunk_size = 500_000\n",
    "svd_sample_size = 100_000\n",
    "n_factors = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495d6368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data for SVD...\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Train SVD on a sample ===\n",
    "print(\"Sampling data for SVD...\")\n",
    "sample_df = pd.read_csv(input_path, usecols=['author_id', 'hotel_name_id', 'rating'], nrows=svd_sample_size)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(sample_df, reader)\n",
    "trainset, _ = surprise_train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b97c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD on sample...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fad9e338130>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = SVD(n_factors=n_factors)\n",
    "print(\"Training SVD on sample...\")\n",
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a6f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    }
   ],
   "source": [
    "# === Step 2: Extract user/hotel embeddings from SVD ===\n",
    "# print(\"Creating embedding lookup tables...\")\n",
    "# user_embeddings = {}\n",
    "# item_embeddings = {}\n",
    "\n",
    "# for uid in sample_df['author_id'].unique():\n",
    "#     try:\n",
    "#         user_embeddings[uid] = svd.pu[trainset.to_inner_uid(uid)]\n",
    "#     except ValueError:\n",
    "#         user_embeddings[uid] = np.zeros(n_factors)\n",
    "\n",
    "# for iid in sample_df['hotel_name_id'].unique():\n",
    "#     try:\n",
    "#         item_embeddings[iid] = svd.qi[trainset.to_inner_iid(iid)]\n",
    "#     except ValueError:\n",
    "#         item_embeddings[iid] = np.zeros(n_factors)\n",
    "print(\"Generating embeddings...\")\n",
    "user_embeddings = {}\n",
    "for uid in sample_df['author_id'].unique():\n",
    "    try:\n",
    "        user_embeddings[uid] = svd.pu[trainset.to_inner_uid(uid)]\n",
    "    except ValueError:\n",
    "        user_embeddings[uid] = np.zeros(n_factors)\n",
    "\n",
    "item_embeddings = {}\n",
    "for iid in sample_df['hotel_name_id'].unique():\n",
    "    try:\n",
    "        item_embeddings[iid] = svd.qi[trainset.to_inner_iid(iid)]\n",
    "    except ValueError:\n",
    "        item_embeddings[iid] = np.zeros(n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ed8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunks and saving to disk...\n"
     ]
    }
   ],
   "source": [
    "# === Step 3: Prepare to process full dataset in chunks ===\n",
    "print(\"Processing chunks and saving to disk...\")\n",
    "columns = ['author_id', 'hotel_name_id', 'rating', 'sentiment_score',\n",
    "           'sleep quality', 'value', 'rooms', 'service', 'cleanliness', 'location']\n",
    "\n",
    "reader = pd.read_csv(input_path, usecols=columns, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6d4ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 65it [12:28, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_dir = os.path.join(current_dir, 'data/processed/chunks')\n",
    "# === Step 4: Process full dataset in chunks and write to disk ===\n",
    "for i, chunk in enumerate(tqdm(reader, desc=\"Processing chunks\")):\n",
    "    for j in range(n_factors):\n",
    "        chunk[f'user_emb_{j}'] = chunk['author_id'].map(lambda x: user_embeddings.get(x, np.zeros(n_factors))[j])\n",
    "        chunk[f'hotel_emb_{j}'] = chunk['hotel_name_id'].map(lambda x: item_embeddings.get(x, np.zeros(n_factors))[j])\n",
    "\n",
    "    user_emb_cols = [f'user_emb_{j}' for j in range(n_factors)]\n",
    "    hotel_emb_cols = [f'hotel_emb_{j}' for j in range(n_factors)]\n",
    "    structured = ['sleep quality', 'value', 'rooms', 'service', 'cleanliness', 'location']\n",
    "    features = ['rating', 'sentiment_score'] + structured + user_emb_cols + hotel_emb_cols\n",
    "\n",
    "    chunk_path = os.path.join(chunk_dir, f'chunk_{i:03d}.csv.gz')\n",
    "    chunk[features].to_csv(chunk_path, index=False, compression='gzip')\n",
    "\n",
    "print(\"All chunks processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba75979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading first chunk for model training...\n"
     ]
    }
   ],
   "source": [
    "# === Step 5: Load processed data and train LightGBM ===\n",
    "print(\"Loading first chunk for model training...\")\n",
    "first_chunk_path = os.path.join(chunk_dir, 'chunk_000.csv.gz')\n",
    "df_train = pd.read_csv(first_chunk_path, compression='gzip')\n",
    "X_train = df_train.drop(columns='rating')\n",
    "y_train = df_train['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccaf43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 2000,\n",
    "    'verbosity': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb38524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 297\n",
      "[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 4.158654\n",
      "Training completed in 0.17 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Training LightGBM...\")\n",
    "start = time.time()\n",
    "model = lgb.train(params, train_data)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Training completed in {elapsed/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ccede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on training chunk...\n",
      "\n",
      "Training Performance Metrics:\n",
      "RMSE: 0.5526\n",
      "MAE:  0.3848\n",
      "R²:   0.7510\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on training chunk...\")\n",
    "y_pred_train = model.predict(X_train)\n",
    "rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"\\nTraining Performance Metrics:\")\n",
    "print(f\"RMSE: {rmse_train:.4f}\")\n",
    "print(f\"MAE:  {mae_train:.4f}\")\n",
    "print(f\"R²:   {r2_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "357fc5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing overall metrics across all chunks...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing overall metrics across all chunks...\")\n",
    "chunk_files = sorted(glob.glob(os.path.join(chunk_dir, 'chunk_*.csv.gz')))\n",
    "y_true_all = []\n",
    "y_pred_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5983746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating chunks: 100%|██████████| 66/66 [03:39<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(chunk_files, desc=\"Evaluating chunks\"):\n",
    "    df = pd.read_csv(path, compression='gzip')\n",
    "    X = df.drop(columns='rating')\n",
    "    y = df['rating']\n",
    "    y_pred = model.predict(X)\n",
    "    y_true_all.extend(y)\n",
    "    y_pred_all.extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f626da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_all = np.array(y_true_all)\n",
    "y_pred_all = np.array(y_pred_all)\n",
    "\n",
    "rmse = mean_squared_error(y_true_all, y_pred_all, squared=False)\n",
    "mae = mean_absolute_error(y_true_all, y_pred_all)\n",
    "r2 = r2_score(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b45283a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Model Performance on Full Dataset:\n",
      "RMSE: 0.5598\n",
      "MAE:  0.3906\n",
      "R²:   0.7420\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOverall Model Performance on Full Dataset:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "317473a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model and embeddings...\n",
      "Done. Model and metrics ready.\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: Save model and embeddings ===\n",
    "print(\"\\nSaving model and embeddings...\")\n",
    "model_dir = os.path.join(current_dir, 'models')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "joblib.dump(model, os.path.join(model_dir, 'hybrid_cf_lightgbm_model.pkl'))\n",
    "\n",
    "with open(os.path.join(model_dir, 'user_embeddings.pkl'), 'wb') as f:\n",
    "    pickle.dump(user_embeddings, f)\n",
    "with open(os.path.join(model_dir, 'hotel_embeddings.pkl'), 'wb') as f:\n",
    "    pickle.dump(item_embeddings, f)\n",
    "\n",
    "print(\"Done. Model and metrics ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
